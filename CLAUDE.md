# Documentor - Claude Code Context

AI-powered PDF document classification and organization tool using vision LLMs via OpenRouter.

## Quick Reference

**Run**: `uv run python main.py <task> <processed_path> [options]` or `documentor <task> ...`
**Install**: `uv pip install -e .`
**Debug classification**: `python scripts/debug_classification.py <pdf_path>`
**Check hashes**: `check-hash <pdf_path>`

## Architecture

### Two-Phase Extraction Pipeline
1. **Phase 1 - Raw Extraction** (`classify_pdf_document`, line 567): Renders first 2 pages as JPEG, sends to LLM with vision, extracts metadata EXACTLY as appears on document
2. **Phase 2 - Normalization** (`normalize_metadata`, line 487): Maps raw values to canonical enums (e.g., "Anthropic, PBC" -> "Anthropic")

### Two-Tier Hashing
- **Fast hash** (`hash_file_fast`, line 348): SHA256 of raw bytes, 8 chars - for quick duplicate filtering
- **Content hash** (`hash_file`, line 359): Renders all pages at 150 DPI, hashes pixel data - detects true duplicates even if PDF metadata differs

### Dynamic Enums
Document types and issuing parties are loaded dynamically from existing metadata JSON files in the processed directory. Falls back to hardcoded lists if directory doesn't exist. Always includes `$UNKNOWN$` sentinel.

## Key Files

| File | Purpose | Key Lines |
|------|---------|-----------|
| `main.py` | Core app (~1666 lines) | Entry: 1578, Tasks: 1167+ |
| `main.py` | Pydantic models | `DocumentMetadataRaw`: 217, `DocumentMetadata`: 238 |
| `main.py` | Classification | `classify_pdf_document`: 567, `normalize_metadata`: 487 |
| `main.py` | Hashing | `hash_file_fast`: 348, `hash_file`: 359 |
| `scripts/debug_classification.py` | Debug LLM calls | Full API response with images |
| `scripts/check_hash.py` | Verify hashes | CLI: `check-hash` |

## Data Models (Pydantic)

```python
DocumentMetadataRaw    # Phase 1: exact text from document
DocumentMetadataInput  # With enum validation
DocumentMetadata       # Full: hashes, timestamps, raw values
```

Fields: `issue_date`, `document_type`, `issuing_party`, `service_name`, `total_amount`, `total_amount_currency`, `confidence`, `reasoning`, `content_hash`, `file_hash`, `create_date`, `update_date`, `document_type_raw`, `issuing_party_raw`

## File Naming Convention

Pattern: `YYYY-MM-DD - document-type - issuing-party - [service] - [amount currency] - hash.pdf`
Example: `2025-01-02 - invoice - anthropic - claude-api - 120 eur - a1b2c3d4.pdf`

Generated by `file_name_from_metadata()` (line 447). All components lowercase, sanitized.

## CLI Tasks

| Task | Purpose | Required Options |
|------|---------|------------------|
| `extract_new` | Process new PDFs from raw folder | `--raw_path` |
| `rename_files` | Rename based on metadata | - |
| `validate_metadata` | Check consistency | - |
| `export_excel` | Export to Excel | `--excel_output_path` |
| `copy_matching` | Copy files matching regex | `--regex_pattern`, `--copy_dest_folder` |
| `export_all_dates` | Export by date range | `--export_base_dir` |
| `check_files_exist` | Validate against schema | `--check_schema_path` (optional) |
| `pipeline` | Full end-to-end workflow | `--export_date` (optional) |

## Configuration

**Location**: `~/.documentor/.env` (auto-created on first run)

```env
OPENROUTER_MODEL_ID=openai/gpt-4.1    # or google/gemini-2.5-flash
OPENROUTER_API_KEY=sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
RAW_FILES_DIR=/path/to/raw;/optional/path2
PROCESSED_FILES_DIR=/path/to/processed
EXPORT_FILES_DIR=/path/to/export
```

Config files in `~/.documentor/`: `file_check_validations.json`, `passwords.txt`

## Code Patterns

- **Progress bars**: Always use `tqdm` for loops over files
- **Error handling**: Log failures to `classification_failures.log` via `failure_logger`
- **Validators**: Pydantic `@field_validator` for normalization (currency symbols, amounts, dates)
- **LLM calls**: Use `tool_choice` for structured output, temperature=0 for determinism
- **Fallbacks**: Always fall back to `$UNKNOWN$` for unrecognized values

## Common Development Tasks

**Add new document type**: Just process documents with that type - it's automatically added from metadata
**Add new issuing party**: Same - dynamically loaded from processed metadata
**Debug classification failure**: `python scripts/debug_classification.py <pdf>` shows full LLM response
**Verify duplicate detection**: `check-hash <pdf>` shows both fast and content hashes

## Testing

No test suite currently. Debug scripts available in `scripts/`.

## Dependencies

Core: `openai`, `PyMuPDF (fitz)`, `pandas`, `pydantic`, `pillow`, `tqdm`, `openpyxl`, `python-dotenv`
Build: `hatchling`, Package manager: `uv`
